{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52994873",
   "metadata": {},
   "source": [
    "VIF results, multicollinearity is severe even after dropping some columns. Here are **advanced solutions** tailored to your data:\n",
    "\n",
    "\n",
    "\n",
    "### **1. Principal Component Analysis (PCA)**\n",
    "**Best for**: Reducing dimensions while preserving information.  \n",
    "**Action**:  \n",
    "- Combine `ssc_p`, `hsc_p`, `degree_p` into 1-2 uncorrelated components.  \n",
    "**Code**:  \n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "X = dataset[['ssc_p', 'hsc_p', 'degree_p', 'etest_p']]\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components\n",
    "X_pca = pca.fit_transform(X)\n",
    "```\n",
    "**Why**: Your VIFs drop drastically (e.g., from 72 → <5) since PCA removes correlations.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Ridge Regression (L2 Regularization)**\n",
    "**Best for**: Keeping all variables but stabilizing coefficients.  \n",
    "**Action**:  \n",
    "- Penalize large coefficients to handle multicollinearity.  \n",
    "**Code**:  \n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=10.0)  # Higher alpha = stronger penalty\n",
    "ridge.fit(X, y)  # X includes all predictors\n",
    "```\n",
    "**Why**: Works well when you can’t drop variables (e.g., all are theoretically important).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Partial Least Squares Regression (PLSR)**\n",
    "**Best for**: Correlated predictors **and** targets.  \n",
    "**Code**:  \n",
    "```python\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(X, y)\n",
    "```\n",
    "**Why**: Unlike PCA, PLSR considers the target variable (`salary`) when reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Feature Aggregation**\n",
    "**Best for**: Variables measuring similar traits (e.g., academic scores).  \n",
    "**Action**:  \n",
    "- Create a composite score:  \n",
    "```python\n",
    "dataset['academic_score'] = dataset[['ssc_p', 'hsc_p', 'degree_p']].mean(axis=1)\n",
    "```\n",
    "**Why**: Combines correlated vars into one, reducing VIF from 72 → ~1-2.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Elastic Net (L1 + L2 Regularization)**\n",
    "**Best for**: Automated variable selection + multicollinearity handling.  \n",
    "**Code**:  \n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "enet.fit(X, y)\n",
    "```\n",
    "**Why**: L1 penalty drops some variables (like `hsc_p`), while L2 handles the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendation for Data**\n",
    "1. **First Try PCA**:  \n",
    "   - Your VIFs for `ssc_p`/`hsc_p`/`degree_p` are extreme (72-116). PCA will resolve this.  \n",
    "2. **If Interpretability Matters**: Use **Ridge Regression** or **Elastic Net**.  \n",
    "3. **For Simplicity**: Aggregate `ssc_p`+`hsc_p`+`degree_p` into one score.\n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Outcome**\n",
    "| Method               | VIF Result           | Notes                          |\n",
    "|----------------------|----------------------|--------------------------------|\n",
    "| PCA                  | All VIFs ≈ 1         | Lose interpretability          |\n",
    "| Ridge Regression     | Coefficients stable  | Keep all variables             |\n",
    "| Feature Aggregation  | VIFs ~1-2            | Simple, retains some meaning   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b57ef",
   "metadata": {},
   "source": [
    "### **Purpose of Homoscedasticity vs. Heteroscedasticity**\n",
    "\n",
    "#### **1. Definitions**\n",
    "- **Homoscedasticity**:  \n",
    "  - The variance of residuals (errors) is **constant** across all levels of the predicted/fitted values.  \n",
    "  - *Ideal for regression models* (OLS assumptions).  \n",
    "\n",
    "- **Heteroscedasticity**:  \n",
    "  - The variance of residuals **changes** (often increases/decreases) with predicted values.  \n",
    "  - *Violates OLS assumptions*, leading to unreliable statistical tests.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Purpose & Importance**\n",
    "| **Aspect**               | **Homoscedasticity**                          | **Heteroscedasticity**                        |\n",
    "|--------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Regression Validity**  | Ensures unbiased, efficient estimates.        | Biases standard errors, affecting p-values.   |\n",
    "| **Confidence Intervals** | Accurate CI/p-values for coefficients.        | CIs become too narrow/wide (misleading).      |\n",
    "| **Model Performance**    | Predictions are equally reliable across data. | Predictions are less reliable for extreme values. |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Thumb Rules**\n",
    "1. **Check Residual Plots**:  \n",
    "   - Plot residuals vs. predicted values.  \n",
    "   - **Homoscedasticity**: Points form a random cloud (no pattern).  \n",
    "   - **Heteroscedasticity**: Fan shape, funnel shape, or systematic trends.  \n",
    "\n",
    "2. **Statistical Tests**:  \n",
    "   - **Breusch-Pagan** or **White Test**: Formal tests for heteroscedasticity (*p < 0.05 indicates heteroscedasticity*).  \n",
    "\n",
    "3. **Fix Heteroscedasticity**:  \n",
    "   - Transform the dependent variable (e.g., `log(y)`).  \n",
    "   - Use robust standard errors (e.g., Huber-White estimator).  \n",
    "   - Apply weighted least squares (WLS).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visual Examples (with Python Code)**\n",
    "\n",
    "#### **Homoscedasticity (Ideal)**\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate homoscedastic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = 2 * X + np.random.normal(0, 1, 100)  # Constant variance\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(X, y - (2 * X), alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title(\"Homoscedasticity: Random Cloud Pattern\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "```\n",
    "**Output**:  \n",
    "![image.png](attachment:image.png)  \n",
    "*(Residuals are evenly scattered around zero with no pattern.)*\n",
    "\n",
    "#### **Heteroscedasticity (Problematic)**\n",
    "```python\n",
    "# Generate heteroscedastic data\n",
    "y_hetero = 2 * X + np.random.normal(0, X, 100)  # Variance increases with X\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(X, y_hetero - (2 * X), alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title(\"Heteroscedasticity: Fan-Shaped Pattern\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "```\n",
    "**Output**:  \n",
    "![image-2.png](attachment:image-2.png) \n",
    "*(Residuals fan out as fitted values increase.)*\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Key Takeaways**\n",
    "- **Homoscedasticity** → Trust regression results.  \n",
    "- **Heteroscedasticity** → Fix it (transformations, robust SEs) or use non-linear models.  \n",
    "- **Always visualize residuals** to diagnose issues.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee34826",
   "metadata": {},
   "source": [
    "### **1. Independent Sample T-Test (Unpaired)**\n",
    "**Use Case**: Compare means of two unrelated groups (e.g., Male vs. Female salaries).  \n",
    "**Key Questions Answered**:  \n",
    "- Is there a statistically significant difference in salaries between genders?  \n",
    "- Which group has higher/lower salaries?  \n",
    "\n",
    "#### **Code Example**:\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Clean data and drop NaN values\n",
    "male = dataset[dataset['gender'] == 'M']['salary'].dropna()\n",
    "female = dataset[dataset['gender'] == 'F']['salary'].dropna()\n",
    "\n",
    "# Perform t-test (use equal_var=False if variances are unequal)\n",
    "t_stat, p_value = ttest_ind(male, female, equal_var=False)\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "male_mean, female_mean = male.mean(), female.mean()\n",
    "male_std, female_std = male.std(), female.std()\n",
    "\n",
    "print(\n",
    "    f\"Independent T-Test Results (Male vs. Female Salary):\\n\"\n",
    "    f\"T-statistic: {t_stat:.3f}, P-value: {p_value:.3f}\\n\"\n",
    "    f\"Male Salary: Mean = {male_mean:.2f}, Std = {male_std:.2f}\\n\"\n",
    "    f\"Female Salary: Mean = {female_mean:.2f}, Std = {female_std:.2f}\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Interpretation**:\n",
    "- **P-value < 0.05**: Significant difference between groups.  \n",
    "- **Effect Size**: Calculate Cohen’s *d* for practical significance:  \n",
    "  ```python\n",
    "  import numpy as np\n",
    "  pooled_std = np.sqrt((male_std**2 + female_std**2) / 2)\n",
    "  cohen_d = (male_mean - female_mean) / pooled_std\n",
    "  print(f\"Cohen's d: {cohen_d:.3f}\")  # Small (0.2), Medium (0.5), Large (0.8)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Dependent Sample T-Test (Paired)**\n",
    "**Use Case**: Compare means of the same group under two conditions (e.g., Male `ssc_p` vs. `hsc_p`).  \n",
    "**Key Questions Answered**:  \n",
    "- Is there a significant change in academic scores (SSC to HSC) for males?  \n",
    "- Did scores improve/decline?  \n",
    "\n",
    "#### **Code Example**:\n",
    "```python\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Ensure paired samples (drop NaN pairs)\n",
    "paired_data = dataset[dataset['gender'] == 'M'][['ssc_p', 'hsc_p']].dropna()\n",
    "male_ssc = paired_data['ssc_p']\n",
    "male_hsc = paired_data['hsc_p']\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = ttest_rel(male_ssc, male_hsc)\n",
    "\n",
    "# Descriptive statistics\n",
    "mean_diff = (male_hsc - male_ssc).mean()\n",
    "std_diff = (male_hsc - male_ssc).std()\n",
    "\n",
    "print(\n",
    "    f\"Paired T-Test Results (Male SSC vs. HSC Scores):\\n\"\n",
    "    f\"T-statistic: {t_stat:.3f}, P-value: {p_value:.3f}\\n\"\n",
    "    f\"Mean Improvement (HSC - SSC): {mean_diff:.2f} ± {std_diff:.2f}\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Interpretation**:\n",
    "- **P-value < 0.05**: Significant change from SSC to HSC.  \n",
    "- **Mean Difference**: Positive value indicates improvement.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Visualizing Results**\n",
    "#### **Boxplot for Independent T-Test**:\n",
    "```python\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='gender', y='salary', data=dataset.dropna())\n",
    "plt.title(\"Salary Distribution by Gender\")\n",
    "plt.show()\n",
    "```\n",
    "![image.png](attachment:image.png) \n",
    "\n",
    "#### **Line Plot for Paired T-Test**:\n",
    "```python\n",
    "plt.plot([0, 1], [male_ssc.mean(), male_hsc.mean()], marker='o', label='Male Scores')\n",
    "plt.xticks([0, 1], ['SSC', 'HSC'])\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.title(\"Academic Score Change (SSC to HSC)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights to Report**\n",
    "| **Test Type**       | **Metric**          | **Interpretation**                                                                 |\n",
    "|----------------------|---------------------|-----------------------------------------------------------------------------------|\n",
    "| Independent T-Test   | P-value, Cohen’s *d* | \"Males earn significantly higher (p=0.02, d=0.4) than females.\"                   |\n",
    "| Dependent T-Test     | P-value, Mean Diff  | \"Male HSC scores improved by 5.2 points (p=0.01) compared to SSC.\"                |\n",
    "\n",
    "---\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a315eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
